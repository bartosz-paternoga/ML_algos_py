{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled19.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AI9i3KKtickN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Tensor (object):\n",
        "    \n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,1000000000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = Tensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    return\n",
        "                    print(self.id)\n",
        "                    print(self.creation_op)\n",
        "                    print(len(self.creators))\n",
        "                    for c in self.creators:\n",
        "                        print(c.creation_op)\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if(self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if(self.creation_op == \"add\"):\n",
        "                    self.creators[0].backward(self.grad, self)\n",
        "                    self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if(self.creation_op == \"sub\"):\n",
        "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
        "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if(self.creation_op == \"mul\"):\n",
        "                    new = self.grad * self.creators[1]\n",
        "                    self.creators[0].backward(new , self)\n",
        "                    new = self.grad * self.creators[0]\n",
        "                    self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if(self.creation_op == \"mm\"):\n",
        "                    c0 = self.creators[0]\n",
        "                    c1 = self.creators[1]\n",
        "                    new = self.grad.mm(c1.transpose())\n",
        "                    c0.backward(new)\n",
        "                    new = self.grad.transpose().mm(c0).transpose()\n",
        "                    c1.backward(new)\n",
        "                    \n",
        "                if(self.creation_op == \"transpose\"):\n",
        "                    self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if(\"sum\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.expand(dim,\n",
        "                                                               self.creators[0].data.shape[dim]))\n",
        "\n",
        "                if(\"expand\" in self.creation_op):\n",
        "                    dim = int(self.creation_op.split(\"_\")[1])\n",
        "                    self.creators[0].backward(self.grad.sum(dim))\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                    self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "                if(self.creation_op == \"sigmoid\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
        "                \n",
        "                if(self.creation_op == \"tanh\"):\n",
        "                    ones = Tensor(np.ones_like(self.grad.data))\n",
        "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
        "                \n",
        "                if(self.creation_op == \"index_select\"):\n",
        "                    new_grad = np.zeros_like(self.creators[0].data)\n",
        "                    indices_ = self.index_select_indices.data.flatten()\n",
        "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
        "                    for i in range(len(indices_)):\n",
        "                        new_grad[indices_[i]] += grad_[i]\n",
        "                    self.creators[0].backward(Tensor(new_grad))\n",
        "                    \n",
        "                if(self.creation_op == \"cross_entropy\"):\n",
        "                    dx = self.softmax_output - self.target_dist\n",
        "                    self.creators[0].backward(Tensor(dx))\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return Tensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return Tensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return Tensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return Tensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return Tensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return Tensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return Tensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return Tensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return Tensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return Tensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return Tensor(self.data.dot(x.data))\n",
        "    \n",
        "    def sigmoid(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sigmoid\")\n",
        "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
        "\n",
        "    def tanh(self):\n",
        "        if(self.autograd):\n",
        "            return Tensor(np.tanh(self.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"tanh\")\n",
        "        return Tensor(np.tanh(self.data))\n",
        "    \n",
        "    def index_select(self, indices):\n",
        "\n",
        "        if(self.autograd):\n",
        "            new = Tensor(self.data[indices.data],\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"index_select\")\n",
        "            new.index_select_indices = indices\n",
        "            return new\n",
        "        return Tensor(self.data[indices.data])\n",
        "    \n",
        "    def softmax(self):\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        return softmax_output\n",
        "    \n",
        "    def cross_entropy(self, target_indices):\n",
        "\n",
        "        temp = np.exp(self.data)\n",
        "        softmax_output = temp / np.sum(temp,\n",
        "                                       axis=len(self.data.shape)-1,\n",
        "                                       keepdims=True)\n",
        "        \n",
        "        t = target_indices.data.flatten()\n",
        "        p = softmax_output.reshape(len(t),-1)\n",
        "        target_dist = np.eye(p.shape[1])[t]\n",
        "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
        "    \n",
        "        if(self.autograd):\n",
        "            out = Tensor(loss,\n",
        "                         autograd=True,\n",
        "                         creators=[self],\n",
        "                         creation_op=\"cross_entropy\")\n",
        "            out.softmax_output = softmax_output\n",
        "            out.target_dist = target_dist\n",
        "            return out\n",
        "\n",
        "        return Tensor(loss)\n",
        "        \n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  \n",
        "\n",
        "class Layer(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.parameters = list()\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return self.parameters\n",
        "\n",
        "    \n",
        "class SGD(object):\n",
        "    \n",
        "    def __init__(self, parameters, alpha=0.1):\n",
        "        self.parameters = parameters\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def zero(self):\n",
        "        for p in self.parameters:\n",
        "            p.grad.data *= 0\n",
        "        \n",
        "    def step(self, zero=True):\n",
        "        \n",
        "        for p in self.parameters:\n",
        "            \n",
        "            p.data -= p.grad.data * self.alpha\n",
        "            \n",
        "            if(zero):\n",
        "                p.grad.data *= 0\n",
        "\n",
        "\n",
        "class Linear(Layer):\n",
        "\n",
        "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.use_bias = bias\n",
        "        \n",
        "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0/(n_inputs))\n",
        "        self.weight = Tensor(W, autograd=True)\n",
        "        if(self.use_bias):\n",
        "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "        \n",
        "        if(self.use_bias):        \n",
        "            self.parameters.append(self.bias)\n",
        "\n",
        "    def forward(self, input):\n",
        "        if(self.use_bias):\n",
        "            return input.mm(self.weight)+self.bias.expand(0,len(input.data))\n",
        "        return input.mm(self.weight)\n",
        "\n",
        "\n",
        "class Sequential(Layer):\n",
        "    \n",
        "    def __init__(self, layers=list()):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = layers\n",
        "    \n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        params = list()\n",
        "        for l in self.layers:\n",
        "            params += l.get_parameters()\n",
        "        return params\n",
        "\n",
        "\n",
        "class Embedding(Layer):\n",
        "    \n",
        "    def __init__(self, vocab_size, dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        \n",
        "        # this random initialiation style is just a convention from word2vec\n",
        "        self.weight = Tensor((np.random.rand(vocab_size, dim) - 0.5) / dim, autograd=True)\n",
        "        \n",
        "        self.parameters.append(self.weight)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return self.weight.index_select(input)\n",
        "\n",
        "\n",
        "class Tanh(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.tanh()\n",
        "\n",
        "\n",
        "class Sigmoid(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return input.sigmoid()\n",
        "    \n",
        "\n",
        "class CrossEntropyLoss(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        return input.cross_entropy(target)\n",
        "\n",
        "    \n",
        "class RNNCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "        \n",
        "        if(activation == 'sigmoid'):\n",
        "            self.activation = Sigmoid()\n",
        "        elif(activation == 'tanh'):\n",
        "            self.activation == Tanh()\n",
        "        else:\n",
        "            raise Exception(\"Non-linearity not found\")\n",
        "\n",
        "        self.w_ih = Linear(n_inputs, n_hidden)\n",
        "        self.w_hh = Linear(n_hidden, n_hidden)\n",
        "        self.w_ho = Linear(n_hidden, n_output)\n",
        "        \n",
        "        self.parameters += self.w_ih.get_parameters()\n",
        "        self.parameters += self.w_hh.get_parameters()\n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        from_prev_hidden = self.w_hh.forward(hidden)\n",
        "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
        "        new_hidden = self.activation.forward(combined)\n",
        "        output = self.w_ho.forward(new_hidden)\n",
        "        return output, new_hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "    \n",
        "class LSTMCell(Layer):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_hidden, n_output):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.xf = Linear(n_inputs, n_hidden)\n",
        "        self.xi = Linear(n_inputs, n_hidden)\n",
        "        self.xo = Linear(n_inputs, n_hidden)        \n",
        "        self.xc = Linear(n_inputs, n_hidden)        \n",
        "        \n",
        "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
        "        self.hc = Linear(n_hidden, n_hidden, bias=False)        \n",
        "        \n",
        "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
        "        \n",
        "        self.parameters += self.xf.get_parameters()\n",
        "        self.parameters += self.xi.get_parameters()\n",
        "        self.parameters += self.xo.get_parameters()\n",
        "        self.parameters += self.xc.get_parameters()\n",
        "\n",
        "        self.parameters += self.hf.get_parameters()\n",
        "        self.parameters += self.hi.get_parameters()        \n",
        "        self.parameters += self.ho.get_parameters()        \n",
        "        self.parameters += self.hc.get_parameters()                \n",
        "        \n",
        "        self.parameters += self.w_ho.get_parameters()        \n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        \n",
        "        prev_hidden = hidden[0]        \n",
        "        prev_cell = hidden[1]\n",
        "        \n",
        "        f = (self.xf.forward(input) + self.hf.forward(prev_hidden)).sigmoid()\n",
        "        i = (self.xi.forward(input) + self.hi.forward(prev_hidden)).sigmoid()\n",
        "        o = (self.xo.forward(input) + self.ho.forward(prev_hidden)).sigmoid()        \n",
        "        g = (self.xc.forward(input) + self.hc.forward(prev_hidden)).tanh()        \n",
        "        c = (f * prev_cell) + (i * g)\n",
        "\n",
        "        h = o * c.tanh()\n",
        "        \n",
        "        output = self.w_ho.forward(h)\n",
        "        return output, (h, c)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        init_hidden = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_cell = Tensor(np.zeros((batch_size,self.n_hidden)), autograd=True)\n",
        "        init_hidden.data[:,0] += 1\n",
        "        init_cell.data[:,0] += 1\n",
        "        return (init_hidden, init_cell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v-afSjh3q81m",
        "colab_type": "code",
        "outputId": "8e1f7a5f-0309-4d8c-9ced-6bf0a199fb2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cSioewlfjYxG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# dataset from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "f = open('gdrive/My Drive/Grokking/shakespear.txt','r')\n",
        "raw = f.read()\n",
        "f.close()\n",
        "\n",
        "vocab = list(set(raw))\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "indices = np.array(list(map(lambda x:word2index[x], raw)))\n",
        "\n",
        "embed = Embedding(vocab_size=len(vocab),dim=512)\n",
        "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
        "\n",
        "batch_size = 32\n",
        "bptt = 16\n",
        "n_batches = int((indices.shape[0] / (batch_size)))\n",
        "\n",
        "trimmed_indices = indices[:n_batches*batch_size]\n",
        "batched_indices = trimmed_indices.reshape(batch_size, n_batches).transpose()\n",
        "\n",
        "input_batched_indices = batched_indices[0:-1]\n",
        "target_batched_indices = batched_indices[1:]\n",
        "\n",
        "n_bptt = int(((n_batches-1) / bptt))\n",
        "input_batches = input_batched_indices[:n_bptt*bptt].reshape(n_bptt,bptt,batch_size)\n",
        "target_batches = target_batched_indices[:n_bptt*bptt].reshape(n_bptt, bptt, batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eM60DQTpjXWJ",
        "colab_type": "code",
        "outputId": "e65f9c3c-67cf-4244-e12a-07499865371e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "def train(iterations=400):\n",
        "    for iter in range(iterations):\n",
        "        total_loss = 0\n",
        "        n_loss = 0\n",
        "\n",
        "        hidden = model.init_hidden(batch_size=batch_size)\n",
        "        for batch_i in range(len(input_batches)):\n",
        "\n",
        "            hidden = Tensor(hidden.data, autograd=True)\n",
        "            loss = None\n",
        "            losses = list()\n",
        "            for t in range(bptt):\n",
        "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
        "                rnn_input = embed.forward(input=input)\n",
        "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "\n",
        "                target = Tensor(target_batches[batch_i][t], autograd=True)    \n",
        "                batch_loss = criterion.forward(output, target)\n",
        "                losses.append(batch_loss)\n",
        "                if(t == 0):\n",
        "                    loss = batch_loss\n",
        "                else:\n",
        "                    loss = loss + batch_loss\n",
        "            for loss in losses:\n",
        "                \"\"\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            total_loss += loss.data\n",
        "            log = \"\\r Iter:\" + str(iter)\n",
        "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
        "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
        "#             if(batch_i == 0):\n",
        "#                 log += \" - \" + generate_sample(n=70, init_char='\\n').replace(\"\\n\",\" \")\n",
        "            if(batch_i % 10 == 0 or batch_i-1 == len(input_batches)):\n",
        "                sys.stdout.write(log)\n",
        "        optim.alpha *= 0.99\n",
        "        print()\n",
        " \n",
        "train(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Iter:0 - Batch 191/195 - Loss:7.669480489227388\n",
            " Iter:1 - Batch 191/195 - Loss:7.420901862788479\n",
            " Iter:2 - Batch 191/195 - Loss:7.174162984882293\n",
            " Iter:3 - Batch 191/195 - Loss:6.934918307023439\n",
            " Iter:4 - Batch 191/195 - Loss:6.703247663069262\n",
            " Iter:5 - Batch 191/195 - Loss:6.47857861534776\n",
            " Iter:6 - Batch 191/195 - Loss:6.25945170528528\n",
            " Iter:7 - Batch 191/195 - Loss:6.045377392791473\n",
            " Iter:8 - Batch 191/195 - Loss:5.83636885573904\n",
            " Iter:9 - Batch 191/195 - Loss:5.6313262148130026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PQ-4IZw_J1Xd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "5c1cf572-16f6-44d9-cf41-16c8362e835f"
      },
      "cell_type": "code",
      "source": [
        "def generate_sample(n=30, init_char=' '):\n",
        "    s = \"\"\n",
        "    hidden = model.init_hidden(batch_size=1)\n",
        "    input = Tensor(np.array([word2index[init_char]]))\n",
        "    for i in range(n):\n",
        "        rnn_input = embed.forward(input)\n",
        "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
        "        output.data *= 10\n",
        "        temp_dist = output.softmax()\n",
        "        temp_dist /= temp_dist.sum()\n",
        "\n",
        "        m = (temp_dist > np.random.rand()).argmax()\n",
        "#         m = output.data.argmax()\n",
        "        c = vocab[m]\n",
        "        input = Tensor(np.array([m]))\n",
        "        s += c\n",
        "    return s\n",
        "print(generate_sample(n=2000, init_char='\\n'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memmy shake shand have shand have shand have shand have shand have shaked: a RRERUS:\n",
            "Ro my shaR I wash have shand have shand have shaR have shand have shand have shand have shake shaR have shand have shand have shandR-ArR HENR HENR HENR HENR HENRR HENRR HENR HENR HENR HENR HENRR HENRR HENRR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENRR HENRR HENR HENR HENR HENR HENR HENR HENR HENR HENRR HENRR HENRR HENRE:\n",
            "TheR he have shakeR\n",
            "The whRers, and ReRt shand have shand have shand have shand have shand have shand have shand have shand have shand have shaR have shand have shand have shand have shand have shand have shake shand have shand have shand have shake shand have shakeR\n",
            "The Ress have shand have shand have shand have shand have shand have shand hRResR have shand have shaR have shand have shand have shand have shand have shaR have shakeR\n",
            "The whe Ress hRer have shake shaR hRer have shaR have shand have shand have shake shaR have shaR have shand have shand have shand have shand have shand have shake shand have shand have shand have shand have shand have shand have shand have shand hR Ress have shand have shand have shakeRR HE HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENR HENRR HENR HENRR HENR HENR HENRR HENR HENR HENR HENR HENR HENR HENR HENR HENRR HENRE:\n",
            "Ro my shandR-Ard, shank have shand have shake shand have shand have shakeR\n",
            "The whRers, shank have shand have shand have shand have shand have shand have shand have shand have shand have shaR have shaR have shaR have shand have shand have shand have shaR have Ress have shake Rere sReRt shand have shand have shand have shand have shand have shand have shand have shand have shand have shand have shaR have shand have shand have shaR have shand have shand have shand have shake shaR Reak the Ress have shand have shaR have shand have shand have shand have shand have shand have shand have shand hRResR have shand have shakeR\n",
            "The Ress wReemmeds, shank have shand have shand have\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}